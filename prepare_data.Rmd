---
title: "mlflx data"
author: "Beni"
date: "1/5/2021"
output: html_document
---


```{r setup, include=FALSE}
library(tidyverse)
library(ingestr)  # https://github.com/stineb/ingestr
library(rbeni)    # https://github.com/stineb/rbeni
```


## Site selection

Site selection was done based on:

1. Site where fLUE method worked. I.e., site belongs to clusters cDD, cGR, cLS, or cNA, as described in Stocker et al. (2018) *New Phytologist*.

*ADDITIONAL SELECTION STEP BELOW NOT APPLIED*
2. Site was among the subset of homogenous sites, selected by Manuela Balzarolo. All sites with a homogenous surrounding are listed in file `~/data/FLUXNET-2015_Tier1/meta/fluxnet_quality_check_homogenous_OK_PRI_mbalzarolo.csv`. 

Reproduce it as follows:

Get sites where fLUE data is available
```{r}
df_sites <- read_csv("~/data/flue/flue_stocker18nphyt.csv")  # dataset available here: https://zenodo.org/record/1158524#.YFn9fkhKjUI
df_sites <- dplyr::select(df_sites, site, cluster) %>% unique()
```

Add meta information for sites
```{r}
df_sites <- df_sites %>% 
  rename(sitename = site) %>% 
  left_join( siteinfo_fluxnet2015, by="sitename" )
```

<!-- Get list of sites with homogenous surrounding and subset fLUE sites based on that list -->
<!-- ```{r} -->
<!-- df_homo <- read_csv("~/data/FLUXNET-2015_Tier1/meta/fluxnet_quality_check_homogenous_OK_PRI_mbalzarolo.csv") -->

<!-- df_flue_sites <- df_flue_sites %>%  -->
<!--   mutate(homogenous_mbalzarolo = sitename %in% df_homo$sitename) -->

<!-- df_sub_homo <- df_flue_sites %>%  -->
<!--   dplyr::filter( homogenous_mbalzarolo & cluster %in% c("cGR", "cDD", "cLS", "cNA") ) -->
<!-- ``` -->

Write to file
```{r}
if (!dir.exists("./data")) system("mkdir data")
write_csv(df_sites, path = "./data/df_sites.csv")
```

Show sites on a map.
```{r}
plot_map_simpl() +
  geom_point(data = df_sites, aes(x = lon, y = lat), color = "red")
ggsave("fig/map_sites_mlflx.png", width = 6, height = 4)
```

## FLUXNET data

### Read data

Original data files are shared with you (students) as a zipped file via Cifex.

```{r message=FALSE}
read_dd_bysite <- function(site){
  dir <- "~/data/FLUXNET-2015_Tier1/20191024/DD"
  filn <- list.files(dir, pattern = paste0("FLX_", site, "_FLUXNET2015_FULLSET_DD_"), full.names = TRUE)
  read_csv(filn)
}
list_df <- purrr::map(as.list(df_sites$sitename), ~read_dd_bysite(.))
names(list_df) <- df_sites$sitename  # this makes it a named list
```

### Clean data

DD-specific
```{r}
## function definition
clean_fluxnet_dd <- function(df){
  
  df %>%

    ## select only the variables we're interested in
    dplyr::select(starts_with("TIMESTAMP"),
           ends_with("_F"),
           USTAR,
           CO2_F_MDS,
           # ends_with("QC"),
           -contains("JSB"),
           starts_with("SWC"),
           GPP_NT_VUT_REF,
           NEE_VUT_REF_QC
           ) %>%

    ## convert to a nice date object
    mutate(TIMESTAMP = lubridate::ymd(TIMESTAMP)) %>%

    ## set bad data to NA for multiple variables
    mutate(
      GPP_NT_VUT_REF_raw = GPP_NT_VUT_REF,
      GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC > 0.8, GPP_NT_VUT_REF, NA)
      ) %>%
    
    ## set all -9999 to NA
    na_if(-9999) %>%

    ## drop QC variables (no longer needed), except NEE_VUT_REF_QC
    dplyr::select(-ends_with("_QC"), NEE_VUT_REF_QC)
}

list_df <- purrr::map(list_df, ~clean_fluxnet_dd(.))
list_df[1]
```

Make it a single flat dataframe. Has now shape 265,175 x 19.
```{r}
ddf <- list_df %>% 
  bind_rows(.id = "sitename")
```

Visualise missing data.
```{r}
library(visdat)
ddf %>% 
  vis_miss(warn_large_data = FALSE)
```
Some data `GPP_NT_VUT_REF` is still missing even after commenting out the l. 102 above. Why?

There seem to be sequences of dates missing, not random individual dates. Plot a few examples (red: unfiltered data, black: filtered using `NEE_VUT_REF_QC > 0.8`, see above):

```{r}
sites <- ddf %>% dplyr::filter(is.na(GPP_NT_VUT_REF)) %>% pull(sitename) %>% unique()
ddf %>% 
  dplyr::filter(sitename %in% sites[1]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF_raw), color = "red") +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF))
```
In this case, it's probably because the data before 2010 is just not available. Best to just drop all missing rows (affects only the head of the data frame).

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[2]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF_raw), color = "red") +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF))
```
In this case, a sequence at the start and another sequence in the middle are missing. The missing rows at the start (head) may just be dropped. The ones in the middle may better be imputed to not lose also the four years data (1999 - 2002). What's the most suitable imputation method? Maybe k-nearest neighbours? I let this decision up to you.

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[3]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF_raw), color = "red") +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF))
```

Remove missing data at the head, and in this case also at the tail.

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[4]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF_raw), color = "red") +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF))
```
This is very weird. Data seems to be missing for some years, but missing cells are not NA but actually have a numeric value (-0.195826000) in this case. You'll detect this by looking at weirdly high frequency of certain values in the data:
```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[4]) %>% 
  group_by(GPP_NT_VUT_REF) %>% 
  summarise(count = n()) %>% 
  arrange(desc(count))
```

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[5]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF_raw), color = "red") +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF))
```
This is so weird that we have to make an educated guess and crudely throw away some data. E.g. drop all rows before mid-2007. 
```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[5] & TIMESTAMP > lubridate::ymd("2007-07-01")) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF_raw), color = "red") +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF))
```

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[7]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF_raw), color = "red") +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF))
```
This is weird, too. Let's just use years 2001 - 2006.
```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[7] & TIMESTAMP > lubridate::ymd("2001-01-01") & TIMESTAMP < lubridate::ymd("2007-01-01")) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF_raw), color = "red") +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF))
```

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[8]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF_raw), color = "red") +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF))
```
Looks good. Just remove missing head.

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[8]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF_raw), color = "red") +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF))
```
Same here.

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[9]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF_raw), color = "red") +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF))
```

Again: weird. Let's just use years 2001-2006 or fill missing dates in between with KNN. Note: In case years after 2006 are dropped, it may still be useful to keep data for years 2012-2014 for validation.

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[9] & TIMESTAMP > lubridate::ymd("2001-01-01") & TIMESTAMP < lubridate::ymd("2007-01-01")) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF_raw), color = "red") +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF))
```

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[10]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF_raw), color = "red") +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF))
```
Again weirdly repeated values in year 2010. Make sure to drop them. 

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[11]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF_raw), color = "red") +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF))
```
This one is very challenging. Consider just the last two years or drop data from this site entirely.

Write to file.
```{r}
write_csv(ddf, path = "data/ddf_fluxnet_mlflx_20210323.csv")
```

## Soil moisture data

`SWC_` data is missing for many dates, in particular for the deeper soil layers. Alternatively, one may use modelled soil water content directly as a predictor or impute missing observed `SWC_` data using (somehow) modeled values (must account for different units, bias, etc.).

`~/data/rsofun_benchmarking/df_output_allsites_v3.3.Rdata` is prepared by `rsofun/benchmarking/tag_v3.3/benchmark_gpp_FLUXNET2015_ensemble_v3.3.Rmd`.

```{r}
ddf <- read_csv("data/ddf_fluxnet_mlflx_20210323.csv")
load("~/data/rsofun_benchmarking/df_output_allsites_v3.3.Rdata")    # loads df_output_allsites
df_soilm <- df_output_allsites %>% 
  unnest(data) %>% 
  dplyr::select(sitename, TIMESTAMP = date, wscal)

## some sites are not available from the modelled dataset:
df_sites$sitename[!(df_sites$sitename %in% unique(df_soilm$sitename))]

## combine
ddf <- ddf %>% 
  left_join(df_soilm, by = c("sitename", "TIMESTAMP"))

## show missing
ddf %>% 
  dplyr::select(GPP_NT_VUT_REF, wscal) %>% 
  vis_miss()
```
Write to file now including column wscal (water scalar, 0 = dry, 1 = 100% water holding capacity of the soil).
```{r}
write_csv(ddf, path = "data/ddf_fluxnet_mlflx_wscal_20210323.csv")
```


<!-- HH-specific -->
<!-- ```{r} -->
<!-- clean_fluxnet_hh <- function(df){ -->

<!--   df <- df %>%  -->

<!--     ## select only the variables we're interested in -->
<!--     dplyr::select( -->
<!--       starts_with("TIMESTAMP"), -->
<!--       ends_with("_F"), -->
<!--       CO2_F_MDS, -->
<!--       PPFD_IN,  -->
<!--       GPP_NT_VUT_REF, -->
<!--       starts_with("SWC_F_MDS"), -->
<!--       NEE_VUT_REF_QC, -->
<!--       USTAR, VPD, RH, -->
<!--       ends_with("QC"), -->
<!--       -contains("JSB"), -->
<!--       NIGHT -->
<!--       ) %>%  -->

<!--     ## convert to nice time object -->
<!--     mutate_at(vars(starts_with("TIMESTAMP_")), ymd_hm) %>%  -->

<!--     ## set bad data to NA for multiple variables -->
<!--     mutate( -->
<!--       GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC %in% c(0,1), GPP_NT_VUT_REF, NA), -->
<!--       TA_F = ifelse(TA_F_QC %in% c(0,1), TA_F, NA), -->
<!--       SW_IN_F = ifelse(SW_IN_F_QC %in% c(0,1), SW_IN_F, NA), -->
<!--       LW_IN_F = ifelse(LW_IN_F_QC %in% c(0,1,2), LW_IN_F, NA),   # relaxing filter criterion -->
<!--       VPD_F = ifelse(VPD_F_QC %in% c(0,1), VPD_F, NA), -->
<!--       PA_F = ifelse(PA_F_QC %in% c(0,1,2), PA_F, NA),   # relaxing filter criterion -->
<!--       P_F = ifelse(P_F_QC %in% c(0,1,2), P_F, NA),   # relaxing filter criterion -->
<!--       WS_F = ifelse(WS_F_QC %in% c(0,1), WS_F, NA), -->
<!--       CO2_F_MDS = ifelse(CO2_F_MDS_QC %in% c(0,1), CO2_F_MDS, NA), -->
<!--       SWC_F_MDS_1 = ifelse(SWC_F_MDS_1_QC %in% c(0,1), SWC_F_MDS_1, NA), -->
<!--       SWC_F_MDS_2 = ifelse(SWC_F_MDS_2_QC %in% c(0,1), SWC_F_MDS_2, NA), -->
<!--       SWC_F_MDS_3 = ifelse(SWC_F_MDS_3_QC %in% c(0,1), SWC_F_MDS_3, NA), -->
<!--       SWC_F_MDS_4 = ifelse(SWC_F_MDS_4_QC %in% c(0,1), SWC_F_MDS_4, NA) -->
<!--       ) %>% -->

<!--     ## set all -9999 to NA -->
<!--     na_if(-9999) %>% -->

<!--     ## drop QC variables (no longer needed), except NEE_VUT_REF_QC -->
<!--     dplyr::select(-ends_with("_QC"), NEE_VUT_REF_QC) -->

<!--   return(df) -->
<!-- } -->
<!-- ``` -->


## MODIS data

### MOD09A1

Get MOD09A1 reflectance data from [ORNL DAAC](https://lpdaac.usgs.gov/products/mod09a1v006/), using the [MODISTools R package](https://github.com/ropensci/MODISTools).

```{r}
library(MODISTools)

## list all products (note MOD09A1)
mt_products()

## list all available bands of MOD09A1
mt_bands(product = "MOD09A1")

## list dates for one site
mt_dates(product = "MOD09A1", lat = df_sites$lat[1], lon = df_sites$lon[1]) %>% 
  pull(calendar_date) %>% 
  range()
```

Get data for all available dates for all of our sites.
```{r}
# test batch download
subsets <- mt_batch_subset(df = df_sites %>% 
                             rename(site_name = sitename) %>% 
                             slice(1:2),
                           product = "MOD09A1",
                           band = c("sur_refl_b01", "sur_refl_b02"),
                           internal = TRUE,
                           start = "2000-02-18",
                           end = "2021-03-01")

```

### fAPAR

This uses the ingestr package to download and interpolate MODIS time series. 
This is also implemented in the download script `rscript_ingest_modis.R`.

```{r}
settings_modis <- get_settings_modis(
  bundle            = "modis_fpar",
  data_path         = "~/data/modis_subsets/",
  method_interpol   = "loess",
  keep              = TRUE,
  overwrite_raw     = FALSE,
  overwrite_interpol= TRUE
  )

df_modis_fpar <- ingest(
  df_sites, 
  source = "modis",
  settings = settings_modis, 
  parallel = FALSE
  # ncores = 2
  )
```

Test plot.
```{r}
plot_fapar_ingestr_bysite(
  df_modis_fpar$data[[64]] %>% 
    tail(1200), 
  settings_modis)
```

Write to file.
```{r}
df_modis_fpar %>% 
  unnest(data) %>% 
  dplyr::select(sitename, date, fpar_loess = loess, fpar_linear = linear) %>% 
  write_csv(path = "data/ddf_fpar_modis_mlflx.csv")
```

## Combine and write

Combine and write to file.
```{r}
ddf <- read_csv("data/ddf_fluxnet_mlflx_wscal_20210323.csv")
df_modis_fpar <- read_csv("data/ddf_fpar_modis_mlflx.csv")

ddf_mlflx <- df_modis_fpar %>% 
  left_join(
    ddf %>% 
      rename(date = TIMESTAMP),
    by = c("sitename", "date")
  ) %>% 
  left_join(df_sites %>% dplyr::select(-year_start, -year_end, -cluster), by = "sitename")

write_csv(ddf_mlflx, path = "data/ddf_combined_mlflx_20210323.csv")
```

Visualise missing data.
```{r}
visdat::vis_miss(ddf_mlflx, warn_large_data = FALSE)
```

Quite many missing fapar data.
```{r}
sites_missing_fpar <- ddf_mlflx %>% 
  dplyr::filter(is.na(fpar_loess)) %>% 
  pull(sitename) %>% 
  unique()

gg <- ddf_mlflx %>% 
  dplyr::filter(sitename %in% sites_missing_fpar) %>% 
  group_by(sitename) %>% 
  nest() %>% 
  mutate(gg = purrr::map(data, ~vis_missing(.))) %>% 
  mutate(gg = purrr::map2(gg, sitename, ~{.x + labs(title = .y)}))

gg$gg[sample(1:nrow(gg), size = 10)]
```

FR-LBr, for example, has much missing fapar data.
```{r}
plot_fapar_ingestr_bysite(
  df_modis_fpar$data[[which(df_modis_fpar$sitename == "FR-LBr")]], 
  settings_modis)
```

Data downloaded (in `raw/`) is only starting in 2002-07-04. Why not from Feb. 2000 (normal modis era start)?
```{r}
library(MODISTools)
mt_dates(product = "MCD15A3H", 
         lat = df_sites %>% dplyr::filter(sitename == "FR-LBr") %>% pull(lat), 
         lon = df_sites %>% dplyr::filter(sitename == "FR-LBr") %>% pull(lon)
         )
```

Ok, yes. Starts only in 2002.
