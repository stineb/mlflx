---
title: "mlflx data"
author: "Beni"
date: "1/5/2021"
output: html_document
---


```{r setup, include=FALSE}
library(tidyverse)
library(ingestr)  # https://github.com/stineb/ingestr
library(rbeni)    # https://github.com/stineb/rbeni
library(recipes)
```


## Site selection

Site selection was done based on:

1. Site where fLUE method worked. I.e., site belongs to clusters cDD, cGR, cLS, or cNA, as described in Stocker et al. (2018) *New Phytologist*.

*ADDITIONAL SELECTION STEP BELOW NOT APPLIED*
2. Site was among the subset of homogenous sites, selected by Manuela Balzarolo. All sites with a homogenous surrounding are listed in file `~/data/FLUXNET-2015_Tier1/meta/fluxnet_quality_check_homogenous_OK_PRI_mbalzarolo.csv`. 

Reproduce it as follows:

Get sites where fLUE data is available
```{r}
df_sites <- read_csv("~/data/flue/flue_stocker18nphyt.csv")  # dataset available here: https://zenodo.org/record/1158524#.YFn9fkhKjUI
df_sites <- dplyr::select(df_sites, site, cluster) %>% unique()
```

Add meta information for sites
```{r}
df_sites <- df_sites %>% 
  rename(sitename = site) %>% 
  left_join( siteinfo_fluxnet2015, by="sitename" )
```

<!-- Get list of sites with homogenous surrounding and subset fLUE sites based on that list -->
<!-- ```{r} -->
<!-- df_homo <- read_csv("~/data/FLUXNET-2015_Tier1/meta/fluxnet_quality_check_homogenous_OK_PRI_mbalzarolo.csv") -->

<!-- df_flue_sites <- df_flue_sites %>%  -->
<!--   mutate(homogenous_mbalzarolo = sitename %in% df_homo$sitename) -->

<!-- df_sub_homo <- df_flue_sites %>%  -->
<!--   dplyr::filter( homogenous_mbalzarolo & cluster %in% c("cGR", "cDD", "cLS", "cNA") ) -->
<!-- ``` -->

Write to file
```{r}
if (!dir.exists("./data")) system("mkdir data")
write_csv(df_sites, path = "./data/df_sites_mlflx.csv")
```

Show sites on a map.
```{r}
plot_map_simpl() +
  geom_point(data = df_sites, aes(x = lon, y = lat), color = "red")
ggsave("fig/map_sites_mlflx.png", width = 6, height = 4)
```

## FLUXNET data

### Read data

Original data files are shared with you (students) as a zipped file via Cifex.

```{r message=FALSE}
read_dd_bysite <- function(site){
  dir <- "~/data/FLUXNET-2015_Tier1/20191024/DD"
  filn <- list.files(dir, pattern = paste0("FLX_", site, "_FLUXNET2015_FULLSET_DD_"), full.names = TRUE)
  read_csv(filn)
}
list_df <- purrr::map(as.list(df_sites$sitename), ~read_dd_bysite(.))
names(list_df) <- df_sites$sitename  # this makes it a named list
```

### Clean data

DD-specific
```{r}
## function definition
clean_fluxnet_dd <- function(df){
  
  df %>%

    # ## select only the variables we're interested in
    # dplyr::select(starts_with("TIMESTAMP"),
    #        ends_with("_F"),
    #        USTAR,
    #        CO2_F_MDS,
    #        -contains("JSB"),
    #        starts_with("SWC"),
    #        GPP_NT_VUT_REF,
    #        NEE_VUT_REF_QC
    #        ) %>%
    
    ## select only the variables we're interested in
    dplyr::select(starts_with("TIMESTAMP"),
           ## predictors
           TA_F, TA_F_DAY, TA_F_NIGHT, SW_IN_F, LW_IN_F, VPD_F, PA_F, P_F, WS_F, starts_with("SWC"),
           
           # ## is often missing 
           # starts_with("PPFD"), starts_with("SW_DIF"), starts_with("SW_OUT"), starts_with("LW_OUT"), starts_with("NETRAD"),
           
           ## targets
           LE_F_MDS, LE_F_MDS_QC, 
           NEE_VUT_REF, NEE_VUT_REF_QC,
           GPP_NT_VUT_REF
           ) %>%    

    ## convert to a nice date object
    mutate(TIMESTAMP = lubridate::ymd(TIMESTAMP)) %>%

    ## set bad data to NA for target variables
    mutate(
      GPP_NT_VUT_REF_raw = GPP_NT_VUT_REF,
      GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC > 0.8, GPP_NT_VUT_REF, NA),
      
      NEE_VUT_REF_raw = NEE_VUT_REF,
      NEE_VUT_REF = ifelse(NEE_VUT_REF_QC > 0.8, NEE_VUT_REF, NA),
      
      # ## cannot be done like this for LE because 25% of LE_F_MDS_QC values are missing
      # LE_F_MDS_raw = LE_F_MDS,
      # LE_F_MDS = ifelse(LE_F_MDS_QC > 0.2, LE_F_MDS, NA)
      
      ) %>%
    
    ## set all -9999 to NA
    na_if(-9999)

    # ## drop QC variables (no longer needed), except NEE_VUT_REF_QC
    # dplyr::select(-ends_with("_QC"))
}

list_df_clean <- purrr::map(list_df, ~clean_fluxnet_dd(.))
list_df_clean[[1]]
```

Additional cleaning step: remove values that appear more than once.

```{r}
identify_pattern <- function( vec ){
  
  spurious_values <- vec %>% 
    table() %>% 
    as_tibble() %>% 
    setNames(c("value", "n")) %>% 
    arrange(desc(n)) %>% 
    dplyr::filter(n>1) %>% 
    pull(value) %>% 
    as.numeric()
  
  vec[vec %in% spurious_values] <- NA
  
  return( vec )
  
}

clean_le <- function(df){
  
  df <- df %>% 
    mutate(LE_F_MDS_raw = LE_F_MDS)
  
  ## clean based on qc only if qc is available
  if (any(!is.na(df$LE_F_MDS_QC)) && ("LE_F_MDS_QC" %in% names(df))){
    df <- df %>% 
      mutate(LE_F_MDS = ifelse(LE_F_MDS_QC > 0.6, LE_F_MDS, NA))
  }
  
  ## remove spurious values
  df <- df %>% 
    mutate(LE_F_MDS = identify_pattern(LE_F_MDS)) %>% 

    ## drop QC variables (no longer needed), except NEE_VUT_REF_QC
    dplyr::select(-ends_with("_QC"))
  
  return(df)
}

list_df_clean <- purrr::map(list_df_clean, ~clean_le(.))
list_df_clean[[1]]
```

<!-- Save cleaned file to remember which data is measured (for model validation). -->
<!-- ```{r} -->
<!-- ddf_which_filled <- list_df_clean %>%  -->
<!--   bind_rows(.id = "sitename") %>%  -->
<!--   dplyr::select(sitename, TIMESTAMP, GPP_NT_VUT_REF, NEE_VUT_REF, LE_F_MDS) %>%  -->
<!--   mutate(filled_GPP_NT_VUT_REF = is.na(GPP_NT_VUT_REF), -->
<!--          filled_NEE_VUT_REF = is.na(NEE_VUT_REF), -->
<!--          filled_LE_F_MDS = is.na(LE_F_MDS)) %>%  -->
<!--   dplyr::select(sitename, TIMESTAMP, starts_with("filled_")) -->
<!-- write_csv(ddf_which_filled, path = "data/ddf_which_filled_mlflx_20210413.csv") -->
<!-- ``` -->

DO NOT GAP-FILL DATA

<!-- ### Gapfill data -->

<!-- Impute missing targets with KNN (k=5) and selected predictors. This is done for each site separately. -->
<!-- ```{r} -->
<!-- library(recipes) -->

<!-- fill_missing_targets <- function(df){ -->

<!--   df <- df %>%  -->
<!--     mutate(LE_F_MDS_clean = LE_F_MDS, NEE_VUT_REF_clean = NEE_VUT_REF, GPP_NT_VUT_REF_clean = GPP_NT_VUT_REF) -->

<!--   pp <- recipe(GPP_NT_VUT_REF ~ ., data = df) %>%  -->

<!--     ## targets -->
<!--     step_knnimpute(LE_F_MDS, neighbors = 5, impute_with = imp_vars(SW_IN_F, LW_IN_F, TA_F, WS_F, VPD_F, P_F)) %>% -->
<!--     step_knnimpute(NEE_VUT_REF, neighbors = 5, impute_with = imp_vars(SW_IN_F, LW_IN_F, TA_F, WS_F, VPD_F, P_F, TA_F_DAY, TA_F_NIGHT)) %>% -->
<!--     step_knnimpute(GPP_NT_VUT_REF, neighbors = 5, impute_with = imp_vars(SW_IN_F, LW_IN_F, TA_F, WS_F, VPD_F, P_F, TA_F_DAY)) %>% -->

<!--     ## this fills only 0.17% -->
<!--     step_knnimpute(TA_F_DAY, neighbors = 5, impute_with = imp_vars(SW_IN_F, LW_IN_F, TA_F)) %>% -->
<!--     step_knnimpute(TA_F_NIGHT, neighbors = 5, impute_with = imp_vars(SW_IN_F, LW_IN_F, TA_F_NIGHT)) %>% -->

<!--     prep(training = df) -->

<!--   df <- bake(pp, new_data = df) -->

<!--   return(df) -->
<!-- } -->

<!-- list_df_clean <- purrr::map(list_df_clean, ~fill_missing_targets(.)) -->

<!-- list_df_clean[[1]] %>%  -->
<!--   ggplot() + -->
<!--   geom_line(aes(TIMESTAMP, LE_F_MDS_raw), color = "red") + -->
<!--   geom_line(aes(TIMESTAMP, LE_F_MDS), color = "royalblue") + -->
<!--   geom_line(aes(TIMESTAMP, LE_F_MDS_clean)) -->

<!-- list_df_clean[[1]] %>%  -->
<!--   ggplot() + -->
<!--   geom_line(aes(TIMESTAMP, NEE_VUT_REF_raw), color = "red") + -->
<!--   geom_line(aes(TIMESTAMP, NEE_VUT_REF), color = "royalblue") + -->
<!--   geom_line(aes(TIMESTAMP, NEE_VUT_REF_clean)) -->

<!-- list_df_clean[[1]] %>%  -->
<!--   ggplot() + -->
<!--   geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF_raw), color = "red") + -->
<!--   geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF), color = "royalblue") + -->
<!--   geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF_clean)) -->
<!-- ``` -->


Make it a single flat dataframe. Has now shape 265,175 x 19.
```{r}
ddf <- list_df_clean %>% 
  bind_rows(.id = "sitename") %>% 
  dplyr::select(-ends_with("_clean"), -ends_with("_raw"))
```

### Visualise missing data

```{r}
load("data/ddf_fluxnet_mlflx_20210510.RData")
ddf %>% 
  visdat::vis_miss(warn_large_data = FALSE)
```

How many missing per site? Remove sites if more than a third of their data is missing.

```{r}
# ddf <- read_csv("data/ddf_fluxnet_mlflx_20210505.csv")

calc_frac_missing <- function(df){
  (df %>% dplyr::filter(is.na(GPP_NT_VUT_REF)) %>% nrow(.))/nrow(df)
}
df_missing <- ddf %>% 
  group_by(sitename) %>% 
  nest() %>% 
  mutate(frac_missing = purrr::map_dbl(data, ~calc_frac_missing(.))) %>% 
  dplyr::select(-data) %>% 
  arrange(desc(frac_missing))

df_missing

sites_missing_gpp <- df_missing %>% 
  dplyr::filter(frac_missing > 0.33333) %>% 
  pull(sitename)

ddf <- ddf %>% 
  dplyr::filter(!(sitename %in% sites_missing_gpp))
```


### Write

Write to file. Now has 211,483 data points.
```{r}
write_csv(ddf, path = "data/ddf_fluxnet_mlflx_20210510.csv")
save(ddf, file = "data/ddf_fluxnet_mlflx_20210510.RData")
```


### Visualise missing GPP

There seem to be sequences of dates missing, not random individual dates. Plot a few examples (red: unfiltered data, black: filtered using `NEE_VUT_REF_QC > 0.8`, see above):

```{r}
sites <- ddf %>% dplyr::filter(is.na(GPP_NT_VUT_REF_raw)) %>% pull(sitename) %>% unique()
ddf %>% 
  dplyr::filter(sitename %in% sites[1]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF_raw), color = "red") +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF))
```

In this case, it's probably because the data before 2010 is just not available. Best to just drop all missing rows (affects only the head of the data frame).

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[2]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF_raw), color = "red") +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF))
```

In this case, a sequence at the start and another sequence in the middle are missing. The missing rows at the start (head) may just be dropped. The ones in the middle may better be imputed to not lose also the four years data (1999 - 2002). What's the most suitable imputation method? Maybe k-nearest neighbours? I let this decision up to you.

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[3]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF_raw), color = "red") +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF))
```

Remove missing data at the head, and in this case also at the tail.

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[4]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF_raw), color = "red") +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF))
```
This is very weird. Data seems to be missing for some years, but missing cells are not NA but actually have a numeric value (-0.195826000) in this case. You'll detect this by looking at weirdly high frequency of certain values in the data:
```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[4]) %>% 
  group_by(GPP_NT_VUT_REF) %>% 
  summarise(count = n()) %>% 
  arrange(desc(count))
```

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[5]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF_raw), color = "red") +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF))
```

This is so weird that we have to make an educated guess and crudely throw away some data. E.g. drop all rows before mid-2007. 

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[5] & TIMESTAMP > lubridate::ymd("2007-07-01")) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF_raw), color = "red") +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF))
```

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[7]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF_raw), color = "red") +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF))
```

This is weird, too. Let's just use years 2001 - 2006.

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[7] & TIMESTAMP > lubridate::ymd("2001-01-01") & TIMESTAMP < lubridate::ymd("2007-01-01")) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF_raw), color = "red") +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF))
```

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[8]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF_raw), color = "red") +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF))
```

Looks good. Just remove missing head.

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[8]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF_raw), color = "red") +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF))
```

Same here.

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[9]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF_raw), color = "red") +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF))
```

Again: weird. Let's just use years 2001-2006 or fill missing dates in between with KNN. Note: In case years after 2006 are dropped, it may still be useful to keep data for years 2012-2014 for validation.

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[9] & TIMESTAMP > lubridate::ymd("2001-01-01") & TIMESTAMP < lubridate::ymd("2007-01-01")) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF_raw), color = "red") +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF))
```

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[10]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF_raw), color = "red") +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF))
```
Again weirdly repeated values in year 2010. Make sure to drop them. 

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[11]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF_raw), color = "red") +
  geom_line(aes(TIMESTAMP, GPP_NT_VUT_REF))
```
This one is very challenging. Consider just the last two years or drop data from this site entirely.

### Visualise missing LE

There seems to be a very bad problem here... Non-sense values 

```{r}
sites <- ddf %>% dplyr::filter(is.na(LE_F_MDS)) %>% pull(sitename) %>% unique()
ddf %>% 
  dplyr::filter(sitename %in% sites[1]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, LE_F_MDS_raw), color = "red") +
  geom_line(aes(TIMESTAMP, LE_F_MDS))
```
In this case, it's probably because the data before 2010 is just not available. Best to just drop all missing rows (affects only the head of the data frame).

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[2]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, LE_F_MDS_raw), color = "red") +
  geom_line(aes(TIMESTAMP, LE_F_MDS))
```
In this case, a sequence at the start and another sequence in the middle are missing. The missing rows at the start (head) may just be dropped. The ones in the middle may better be imputed to not lose also the four years data (1999 - 2002). What's the most suitable imputation method? Maybe k-nearest neighbours? I let this decision up to you.

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[3]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, LE_F_MDS_raw), color = "red") +
  geom_line(aes(TIMESTAMP, LE_F_MDS))
```

Remove missing data at the head, and in this case also at the tail.

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[4]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, LE_F_MDS_raw), color = "red") +
  geom_line(aes(TIMESTAMP, LE_F_MDS))
```
This is very weird. Data seems to be missing for some years, but missing cells are not NA but actually have a numeric value (-0.195826000) in this case. You'll detect this by looking at weirdly high frequency of certain values in the data:
```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[4]) %>% 
  group_by(LE_F_MDS) %>% 
  summarise(count = n()) %>% 
  arrange(desc(count))
```

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[5]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, LE_F_MDS_raw), color = "red") +
  geom_line(aes(TIMESTAMP, LE_F_MDS))
```
This is so weird that we have to make an educated guess and crudely throw away some data. E.g. drop all rows before mid-2007. 
```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[5] & TIMESTAMP > lubridate::ymd("2007-07-01")) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, LE_F_MDS_raw), color = "red") +
  geom_line(aes(TIMESTAMP, LE_F_MDS))
```

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[7]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, LE_F_MDS_raw), color = "red") +
  geom_line(aes(TIMESTAMP, LE_F_MDS))
```

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[8]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, LE_F_MDS_raw), color = "red") +
  geom_line(aes(TIMESTAMP, LE_F_MDS))
```
Looks good. Just remove missing head.

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[8]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, LE_F_MDS_raw), color = "red") +
  geom_line(aes(TIMESTAMP, LE_F_MDS))
```
Same here.

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[9]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, LE_F_MDS_raw), color = "red") +
  geom_line(aes(TIMESTAMP, LE_F_MDS))
```

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[10]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, LE_F_MDS_raw), color = "red") +
  geom_line(aes(TIMESTAMP, LE_F_MDS))
```
Again weirdly repeated values in year 2010. Make sure to drop them. 

```{r}
ddf %>% 
  dplyr::filter(sitename %in% sites[11]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, LE_F_MDS_raw), color = "red") +
  geom_line(aes(TIMESTAMP, LE_F_MDS))
```
This one is very challenging. Consider just the last two years or drop data from this site entirely.


## Soil moisture data

`SWC_` data is missing for many dates, in particular for the deeper soil layers. Alternatively, one may use modelled soil water content directly as a predictor or impute missing observed `SWC_` data using (somehow) modeled values (must account for different units, bias, etc.).

`~/data/rsofun_benchmarking/df_output_allsites_v3.3.Rdata` is prepared by `rsofun/benchmarking/tag_v3.3/benchmark_gpp_FLUXNET2015_ensemble_v3.3.Rmd`.

```{r}
# load("data/ddf_fluxnet_mlflx_20210413.RData")
# df_sites <- read_csv("./data/df_sites_mlflx.csv")

ddf <- read_csv("data/ddf_fluxnet_mlflx_20210510.csv")

df_sites <- ddf %>% 
  mutate(year = lubridate::year(TIMESTAMP)) %>% 
  group_by(sitename) %>% 
  summarise(year_start = min(year), year_end = max(year)) %>% 
  left_join(df_sites %>% dplyr::select(sitename, lon, lat),
            by = "sitename")

load("~/data/rsofun_benchmarking/df_output_allsites_v3.3.Rdata")    # loads df_output_allsites
df_soilm <- df_output_allsites %>% 
  unnest(data) %>% 
  dplyr::select(sitename, TIMESTAMP = date, wscal)

## combine
ddf <- ddf %>% 
  left_join(
    df_soilm %>% dplyr::filter(!(sitename %in% missing_sites_soilm)), 
    by = c("sitename", "TIMESTAMP")
    )
```

Exclude sites for which wscal data is completely missing. Down to 208,195 data points, 54 sites.
```{r}
calc_frac_missing_wscal <- function(df){
  (df %>% dplyr::filter(is.na(wscal)) %>% nrow(.))/nrow(df)
}
sites_missing_wscal <- ddf %>% 
  group_by(sitename) %>% 
  nest() %>% 
  mutate(frac_missing = purrr::map_dbl(data, ~calc_frac_missing_wscal(.))) %>% 
  dplyr::select(-data) %>% 
  dplyr::filter(frac_missing == 1.0) %>% 
  pull(sitename)
ddf <- ddf %>% 
  dplyr::filter(!(sitename %in% sites_missing_wscal))

## update site list data frame
df_sites <- ddf %>% 
  mutate(year = lubridate::year(TIMESTAMP)) %>% 
  group_by(sitename) %>% 
  summarise(year_start = min(year), year_end = max(year))
```


At what sites is wscal missing? Pretty much at all sites at least a few individual dates. Interpolate few missing dates at these sites.
```{r}
ddf_raw <- ddf

interpolate_wscal <- function(df){
  if (any(!is.na(df$wscal))){
    df <- df %>% 
      mutate(wscal = myapprox(wscal))
  }
  return(df)
}

## interpolate by site
ddf <- ddf %>% 
  group_by(sitename) %>% 
  group_split() %>% 
  purrr::map(~interpolate_wscal(.))
names(ddf) <- df_sites$sitename
ddf <- ddf %>% 
  bind_rows(.id = "sitename")
```

<!-- Now fill missing SWC data -->
<!-- ```{r} -->
<!-- source("R/fill_missing_swc.R") -->
<!-- ddf <- ddf %>%  -->
<!--   group_by(sitename) %>%  -->
<!--   group_split() %>%  -->
<!--   purrr::map(~fill_missing_swc(.)) -->
<!-- names(ddf) <- df_sites$sitename -->
<!-- ddf <- ddf %>%  -->
<!--   bind_rows(.id = "sitename") -->

<!-- ddf <- ddf %>%  -->
<!--   dplyr::select(-SWC_F_MDS_5, -SWC_F_MDS_6, -SWC_F_MDS_7) -->
<!-- ``` -->

```{r}
ddf <- ddf %>% 
  dplyr::select(-starts_with("SWC_"))

## show missing
ddf %>% 
  # dplyr::select(starts_with("SWC_"), wscal) %>% 
  visdat::vis_miss(warn_large_data = F)
```

Write to file now including column wscal (water scalar, 0 = dry, 1 = 100% water holding capacity of the soil).
```{r}
write_csv(ddf, path = "data/ddf_fluxnet_mlflx_wscal_20210510.csv")
save(ddf, file = "data/ddf_fluxnet_mlflx_wscal_20210510.RData")
```

Plot a few.
```{r}
ddf %>% 
  dplyr::filter(sitename %in% df_sites$sitename[8]) %>% 
  ggplot() +
  geom_line(aes(TIMESTAMP, wscal))
  # geom_line(aes(TIMESTAMP, SWC_F_MDS_1), color = "royalblue", alpha = 0.5)
```

<!-- HH-specific -->
<!-- ```{r} -->
<!-- clean_fluxnet_hh <- function(df){ -->

<!--   df <- df %>%  -->

<!--     ## select only the variables we're interested in -->
<!--     dplyr::select( -->
<!--       starts_with("TIMESTAMP"), -->
<!--       ends_with("_F"), -->
<!--       CO2_F_MDS, -->
<!--       PPFD_IN,  -->
<!--       GPP_NT_VUT_REF, -->
<!--       starts_with("SWC_F_MDS"), -->
<!--       NEE_VUT_REF_QC, -->
<!--       USTAR, VPD, RH, -->
<!--       ends_with("QC"), -->
<!--       -contains("JSB"), -->
<!--       NIGHT -->
<!--       ) %>%  -->

<!--     ## convert to nice time object -->
<!--     mutate_at(vars(starts_with("TIMESTAMP_")), ymd_hm) %>%  -->

<!--     ## set bad data to NA for multiple variables -->
<!--     mutate( -->
<!--       GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC %in% c(0,1), GPP_NT_VUT_REF, NA), -->
<!--       TA_F = ifelse(TA_F_QC %in% c(0,1), TA_F, NA), -->
<!--       SW_IN_F = ifelse(SW_IN_F_QC %in% c(0,1), SW_IN_F, NA), -->
<!--       LW_IN_F = ifelse(LW_IN_F_QC %in% c(0,1,2), LW_IN_F, NA),   # relaxing filter criterion -->
<!--       VPD_F = ifelse(VPD_F_QC %in% c(0,1), VPD_F, NA), -->
<!--       PA_F = ifelse(PA_F_QC %in% c(0,1,2), PA_F, NA),   # relaxing filter criterion -->
<!--       P_F = ifelse(P_F_QC %in% c(0,1,2), P_F, NA),   # relaxing filter criterion -->
<!--       WS_F = ifelse(WS_F_QC %in% c(0,1), WS_F, NA), -->
<!--       CO2_F_MDS = ifelse(CO2_F_MDS_QC %in% c(0,1), CO2_F_MDS, NA), -->
<!--       SWC_F_MDS_1 = ifelse(SWC_F_MDS_1_QC %in% c(0,1), SWC_F_MDS_1, NA), -->
<!--       SWC_F_MDS_2 = ifelse(SWC_F_MDS_2_QC %in% c(0,1), SWC_F_MDS_2, NA), -->
<!--       SWC_F_MDS_3 = ifelse(SWC_F_MDS_3_QC %in% c(0,1), SWC_F_MDS_3, NA), -->
<!--       SWC_F_MDS_4 = ifelse(SWC_F_MDS_4_QC %in% c(0,1), SWC_F_MDS_4, NA) -->
<!--       ) %>% -->

<!--     ## set all -9999 to NA -->
<!--     na_if(-9999) %>% -->

<!--     ## drop QC variables (no longer needed), except NEE_VUT_REF_QC -->
<!--     dplyr::select(-ends_with("_QC"), NEE_VUT_REF_QC) -->

<!--   return(df) -->
<!-- } -->
<!-- ``` -->


## MODIS data

### MCD43A4

Get MCD43A4 nadir reflectance data from [ORNL DAAC](https://lpdaac.usgs.gov/products/mod09a1v006/).

Using ingestr:

```{r}
load("data/ddf_fluxnet_mlflx_wscal_20210505.RData")
df_sites <- read_csv("./data/df_sites_mlflx.csv")

## create df_sites based on ddf to make sure we get the right dates matching the fluxnet data we now have nicely processed
df_sites_ddf <- ddf %>%
  mutate(year = lubridate::year(TIMESTAMP)) %>%
  group_by(sitename) %>%
  summarise(year_start = min(year), year_end = max(year)) %>%
  left_join(df_sites %>% dplyr::select(sitename, lon, lat),
            by = "sitename")

settings_modis <- get_settings_modis(
  bundle            = "modis_refl",
  data_path         = "~/data/modis_subsets/",
  method_interpol   = "loess",
  keep              = TRUE,
  overwrite_raw     = FALSE,
  overwrite_interpol= TRUE,
  n_focal           = 0,
  network           = "FLUXNET"
  )

df_modis_fpar <- ingest(
  df_sites_ddf %>% 
    slice(1:2) %>% 
    mutate(year_start = 2000, year_end = 2020),
  source = "modis",
  settings = settings_modis, 
  parallel = FALSE
  )
```


Using the [MODISTools R package](https://github.com/ropensci/MODISTools):

```{r}
library(MODISTools)

## list all products (note MOD09A1)
mt_products()

## list all available bands of MOD09A1
mt_bands(product = "MOD09A1")

## list dates for one site
mt_dates(product = "MOD09A1", lat = df_sites$lat[1], lon = df_sites$lon[1]) %>% 
  pull(calendar_date) %>% 
  range()
```

Get data for all available dates for all of our sites.
```{r}
## single site download
df_sites <- read_csv("./data/df_sites.csv")
df_test <- mt_subset(product = "MOD09A1",
                    lat = df_sites$lat[1],
                    lon = df_sites$lon[1],
                    band = c("sur_refl_b01", "sur_refl_b02", "sur_refl_qc_500m", "sur_refl_state_500m", "sur_refl_raz", "sur_refl_szen", "sur_refl_vzen"),
                    start = "2010-01-01",
                    end = "2010-01-01",
                    km_lr = 1,
                    km_ab = 1,
                    site_name = "testsite",
                    internal = TRUE,
                    progress = TRUE
                    ) %>% 
  as_tibble()

```

### fAPAR

This uses the ingestr package to download and interpolate MODIS time series. 
This is also implemented in the download script `rscript_ingest_modis.R`.

```{r}
df_sites <- read_csv("./data/df_sites_mlflx.csv")

## create df_sites based on ddf to make sure we get the right dates matching the fluxnet data we now have nicely processed
df_sites_ddf <- ddf %>% 
  mutate(year = lubridate::year(TIMESTAMP)) %>% 
  group_by(sitename) %>% 
  summarise(year_start = min(year), year_end = max(year)) %>% 
  left_join(df_sites %>% dplyr::select(sitename, lon, lat),
            by = "sitename")

settings_modis <- get_settings_modis(
  bundle            = "modis_fpar",
  data_path         = "~/data/modis_subsets/",
  method_interpol   = "loess",
  keep              = TRUE,
  overwrite_raw     = FALSE,
  overwrite_interpol= TRUE,
  n_focal           = 0,
  filename_with_year= FALSE
  )

df_modis_fpar <- ingest(
  df_sites_ddf,
  # df_sites_ddf %>% 
  #   dplyr::filter(sitename %in% sites_missing_fapar) %>% 
  #   mutate(year_start = 2000, year_end = 2020), 
  source = "modis",
  settings = settings_modis, 
  parallel = FALSE,
  # ncores = 2
  )
```

Gapfill data with mean seasonality.
```{r}
fill_missing_fapar <- function(df){
  
  ## get mean sesasonality
  df_meandoy <- df %>% 
    mutate(doy = lubridate::yday(date)) %>% 
    group_by(doy) %>% 
    summarise(modisvar_meandoy = mean(modisvar_filled, na.rm = TRUE))
  
  ## add to daily
  df %>% 
    mutate(doy = lubridate::yday(date)) %>% 
    left_join(df_meandoy, by = "doy") %>% 
    mutate(modisvar_filled = ifelse(is.na(modisvar_filled), modisvar_meandoy, modisvar_filled)) %>% 
    dplyr::select(-doy, modisvar_meandoy)
  
}
df_modis_fpar <- df_modis_fpar %>% 
  mutate(data = purrr::map(data, ~fill_missing_fapar(.)))
```

Exclude sites for which fapar data is completely missing.
```{r}
calc_frac_missing <- function(df){
  (df %>% dplyr::filter(is.na(modisvar_filled)) %>% nrow(.))/nrow(df)
}
sites_missing_fapar <- df_modis_fpar %>% 
  mutate(frac_missing = purrr::map_dbl(data, ~calc_frac_missing(.))) %>% 
  dplyr::filter(frac_missing == 1.0) %>% 
  pull(sitename)
df_modis_fpar <- df_modis_fpar %>% 
  dplyr::filter(!(sitename %in% sites_missing_fapar))
```

Linearly approximate very few remaining. That's not changing anything. They are missing at head or tail. Just cut it after interpolation.
```{r}
interpolate_modisvar_filled <- function(df){
  if (any(!is.na(df$modisvar_filled))){
    df <- df %>% 
      mutate(modisvar_filled = myapprox(modisvar_filled))
  }
  return(df)
}

## interpolate by site
df_modis_fpar <- df_modis_fpar %>% 
  mutate(data = purrr::map(data, ~interpolate_modisvar_filled(.)))

df_modis_fpar %>% 
  mutate(frac_missing = purrr::map_dbl(data, ~calc_frac_missing(.))) %>% 
  arrange(desc(frac_missing))

## select variable
df_modis_fpar <- df_modis_fpar %>% 
  unnest(data) %>% 
  dplyr::select(sitename, date, fpar = modisvar_filled)

## check what dates are missing - always last of year
df_modis_fpar %>% 
  dplyr::filter(is.na(fpar))

## remove missing dates
df_modis_fpar <- df_modis_fpar %>% 
  drop_na()

## now they are all non-missing. Nice.
df_modis_fpar %>% 
  visdat::vis_miss(warn_large_data = F)
```

Write to file.
```{r}
write_csv(df_modis_fpar, path = "data/ddf_fpar_modis_mlflx_20210510.csv")
save(df_modis_fpar, file = "data/ddf_fpar_modis_mlflx_20210510.RData")
```

## Combine and write

Combine and write to file.
```{r}
# ddf <- read_csv("data/ddf_fluxnet_mlflx_wscal_20210413.csv")
# df_modis_fpar <- read_csv("data/ddf_fpar_modis_mlflx_20210414.csv")

ddf_mlflx <- ddf %>% 
  rename(date = TIMESTAMP) %>% 
  left_join(
    df_modis_fpar,
    by = c("sitename", "date")
  )

ddf_mlflx %>% 
  visdat::vis_miss(warn_large_data = F)

ddf_mlflx %>% 
  dplyr::filter(is.na(fpar))

## at this stage, only 31.12. dates missing - drop them
ddf_mlflx <- ddf_mlflx %>% 
  dplyr::filter(!is.na(fpar))

ddf_mlflx <- ddf_mlflx %>% 

  ## add meta info (time invariant)
  left_join(df_sites %>% dplyr::select(-year_start, -year_end, -cluster), by = "sitename")

write_csv(ddf_mlflx, path = "data/ddf_combined_mlflx_20210510.csv")
```

Visualise missing data.
```{r}
visdat::vis_miss(ddf_mlflx %>% dplyr::select(1:16), warn_large_data = FALSE)
```

