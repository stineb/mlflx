{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dd43691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from model.model import EncoderWithTime, Reparametrize, DecoderNoTime, Regressor, Model\n",
    "from sklearn.metrics import r2_score\n",
    "from preprocess import prepare_df, normalize\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "945f6c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "device = torch.device(\"cuda:\" + str(1))\n",
    "data = pd.read_csv('../lstm/utils/df_imputed.csv', index_col=0).drop(columns =\n",
    "                                                    ['date','TA_F_DAY','TA_F_NIGHT','P_F'])\n",
    "raw = pd.read_csv('../data/df_20210510.csv', index_col=0)['GPP_NT_VUT_REF']\n",
    "df_sensor, df_meta, df_gpp = prepare_df(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f0b442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "sites = raw.index.unique()\n",
    "masks = []\n",
    "for s in sites:\n",
    "    mask = raw[raw.index == s].isna().values\n",
    "    masks.append(list(map(operator.not_, mask)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72dc141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_to_train = list(range(len(df_sensor)))\n",
    "sites_to_train.remove(0)\n",
    "sites_to_test = [0]\n",
    "\n",
    "x_train = [df_sensor[i].values for i in sites_to_train]\n",
    "conditional_train = [df_meta[i].values for i in sites_to_train]\n",
    "y_train = [df_gpp[i].values.reshape(-1,1) for i in sites_to_train]\n",
    "\n",
    "x_test = [df_sensor[i].values for i in sites_to_test]\n",
    "conditional_test = [df_meta[i].values for i in sites_to_test]\n",
    "y_test = [df_gpp[i].values.reshape(-1,1) for i in sites_to_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b45948c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch, math\n",
    "import time\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    # d_model : number of features\n",
    "    def __init__(self,feature_size= 8,num_layers=3,dropout=0):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=4, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)        \n",
    "        self.decoder = nn.Linear(feature_size,1)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1    \n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, device):\n",
    "        \n",
    "        mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "        output = self.transformer_encoder(src,mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31fb0060",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \n",
      "Train loss: 0.64 | R2: 0.36\n",
      "Test loss: 0.93 | R2: 0.10\n",
      "Epoch: 2 \n",
      "Train loss: 0.58 | R2: 0.42\n",
      "Test loss: 1.02 | R2: 0.01\n",
      "Epoch: 3 \n",
      "Train loss: 0.51 | R2: 0.49\n",
      "Test loss: 0.88 | R2: 0.15\n",
      "Epoch: 4 \n",
      "Train loss: 0.44 | R2: 0.56\n",
      "Test loss: 0.75 | R2: 0.28\n",
      "Epoch: 5 \n",
      "Train loss: 0.40 | R2: 0.60\n",
      "Test loss: 0.71 | R2: 0.33\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-f68d5e3b4b4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-2c7865d15218>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, device)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_square_subsequent_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-2c7865d15218>\u001b[0m in \u001b[0;36m_generate_square_subsequent_mask\u001b[0;34m(self, sz)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_generate_square_subsequent_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model = Transformer(feature_size=8).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = torch.nn.MSELoss()\n",
    "best_model = \"\"\n",
    "min_train_loss = float('inf')\n",
    "\n",
    "for epoch in range(100):\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        train_r2 = 0\n",
    "        ## TRAIN -- TEACHER FORCING\n",
    "        model.train()\n",
    "        for (x, y) in zip(x_train, y_train):\n",
    "            optimizer.zero_grad()\n",
    "            x = torch.FloatTensor(x).unsqueeze(1).to(device)\n",
    "            y = torch.FloatTensor(y).to(device)\n",
    "            y_pred = model(x,device).squeeze(1)\n",
    "            loss = criterion( y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.detach().item()\n",
    "            train_r2 += r2_score(y_true=y.detach().cpu().numpy(), y_pred=y_pred.detach().cpu().numpy())\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "              for (x, y) in zip(x_test, y_test):\n",
    "                x = torch.FloatTensor(x).unsqueeze(1).to(device)\n",
    "                y = torch.FloatTensor(y).to(device)\n",
    "                y_pred = model(x,device).squeeze(1)\n",
    "                test_loss = criterion(y_pred, y)\n",
    "                test_r2 = r2_score(y_true=y.detach().cpu()[masks[0]], y_pred=y_pred.detach().cpu()[masks[0]])\n",
    "\n",
    "\n",
    "        print(f\"Epoch: {epoch+1} \")\n",
    "        print(f\"Train loss: {train_loss / len(sites_to_train):.2f} | R2: {train_r2 / len(sites_to_train):.2f}\")\n",
    "        print(f\"Test loss: {test_loss:.2f} | R2: {test_r2:.2f}\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb19e438",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0adee636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 12 19:45:47 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN X (Pascal)    Off  | 00000000:04:00.0 Off |                  N/A |\n",
      "| 71%   83C    P2   234W / 250W |  11855MiB / 12196MiB |     89%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  TITAN X (Pascal)    Off  | 00000000:05:00.0 Off |                  N/A |\n",
      "| 27%   46C    P2    63W / 250W |   6071MiB / 12196MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  TITAN X (Pascal)    Off  | 00000000:08:00.0 Off |                  N/A |\n",
      "| 25%   44C    P2    75W / 250W |    749MiB / 12196MiB |     23%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  TITAN X (Pascal)    Off  | 00000000:09:00.0 Off |                  N/A |\n",
      "| 23%   23C    P8     9W / 250W |      2MiB / 12196MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  TITAN X (Pascal)    Off  | 00000000:83:00.0 Off |                  N/A |\n",
      "| 48%   82C    P2   234W / 250W |  11855MiB / 12196MiB |     88%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  TITAN X (Pascal)    Off  | 00000000:84:00.0 Off |                  N/A |\n",
      "| 23%   35C    P8     9W / 250W |      2MiB / 12196MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  TITAN X (Pascal)    Off  | 00000000:87:00.0 Off |                  N/A |\n",
      "| 23%   38C    P2    84W / 250W |   5176MiB / 12196MiB |     16%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  TITAN X (Pascal)    Off  | 00000000:88:00.0 Off |                  N/A |\n",
      "| 38%   63C    P2   207W / 250W |  10159MiB / 12196MiB |     65%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   2093475      C   python3                         11853MiB |\n",
      "|    1   N/A  N/A   2094447      C   ...3/envs/pytorch/bin/python     6069MiB |\n",
      "|    2   N/A  N/A   1774370      C   python                            747MiB |\n",
      "|    4   N/A  N/A   2095250      C   python3                         11853MiB |\n",
      "|    6   N/A  N/A    819812      C   python                           4549MiB |\n",
      "|    6   N/A  N/A   1875642      C   python                            625MiB |\n",
      "|    7   N/A  N/A   1753546      C   python                          10157MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffac1780",
   "metadata": {},
   "outputs": [],
   "source": [
    "|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
