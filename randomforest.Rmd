---
title: "Random forest"
author: "Beni Stocker"
date: "5/22/2021"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(patchwork)
library(ggrepel)
library(yardstick)
library(rbeni)    # get it by devtools::install_github("https://github.com/stineb/rbeni.git")
library(ingestr)  # get it by devtools::install_github("https://github.com/computationales/ingestr.git")
library(ranger)
```

## 

### Read in

```{r}
df <- read_csv("data/ddf_combined_mlflx_20210510.csv")
```
Specify model
```{r}
trgt <- "GPP_NT_VUT_REF"

## predictors excluding PHO, and TS (too many missing)
preds <- c("TA_F", "TA_F_DAY", "TA_F_NIGHT", "SW_IN_F", "LW_IN_F", "VPD_F", "PA_F", "P_F", "WS_F", "wscal", "fpar", "elv", "classid", "c4", "whc", "igbp_land_use")
```

## Train a model

### Ranger

A random forest model using the ranger library.

Out of the box, it gets R2 = 0.46.
```{r}
mod_rf <- ranger(
  GPP_NT_VUT_REF ~ ., 
  data = df %>% 
    dplyr::select(GPP_NT_VUT_REF, all_of(preds)) %>% 
    drop_na(),
  mtry = floor(length(preds) / 3),
  respect.unordered.factors = "order",
  seed = 123
)

## RMSE and R2
sqrt(mod_rf$prediction.error)
mod_rf$r.squared
```

Does it perform better without `PBR` as predictor (too limiting because of too many missing data points)? This leaves 6859 points in the dataset (as opposed to 5922 points when PBR is included).
Yes! It works better, indeed: R2 = 0.48.
```{r}
mod_rf_noPBR <- ranger(
  leafN ~ ., 
  data = dfs %>% 
    dplyr::select(leafN, all_of(preds)) %>% 
    dplyr::select(-PBR) %>% 
    drop_na(),
  mtry = floor((length(preds)-1) / 3),
  respect.unordered.factors = "order",
  seed = 123
)

## RMSE and R2
sqrt(mod_rf_noPBR$prediction.error)
mod_rf_noPBR$r.squared
```

### Caret Random Forest

Using the caret library with hyperparameter based on best results from above. This is useful to get CV results.
Slightly better results without imputation, instead of dropping data.
```{r}
## create generic formula for the model and define preprocessing steps
pp <- recipe(GPP_NT_VUT_REF ~ ., data = dplyr::select(df, GPP_NT_VUT_REF, preds)) %>%
  
  ## impute by median as part of the recipe
  step_medianimpute(all_predictors())
  # step_impute_median(all_predictors())
  

traincotrlParams <- trainControl( 
  method="cv", 
  number=10, 
  verboseIter=FALSE,
  savePredictions = "final"
  )

tune_grid <- expand.grid( .mtry = c(3,5,7,9), 
                          .min.node.size = c(100, 500, 1000, 2000),
                          .splitrule = "variance"
                          )

set.seed(1982)

mod_rf_caret <- train(
  pp,
  data            = dplyr::select(df, GPP_NT_VUT_REF, preds),
  metric          = "RMSE",
  method          = "ranger",
  tuneGrid        = tune_grid,
  trControl       = traincotrlParams,
  replace         = TRUE,
  sample.fraction = 0.5,
  num.trees       = 2000,        # boosted for the final model
  importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
  )

mod_rf_caret_leafn_noimpute <- train(
  leafN ~ .,
  data = dfs %>%
      dplyr::select(leafN, preds) %>%
      dplyr::select(-PBR) %>%
      drop_na(),
  metric          = "RMSE",
  method          = "ranger",
  tuneGrid        = tune_grid,
  trControl       = traincotrlParams,
  replace         = best_hyper$replace,
  sample.fraction = best_hyper$sample.fraction,
  num.trees       = 2000,        # boosted for the final model
  importance      = "impurity"   # for variable importance analysis, alternative: "permutation"
  )

mod_rf_caret_leafn
mod_rf_caret_leafn_noimpute
```

Visualise cross-validation results using results from the best tuned model.
```{r}
## get predicted values from cross-validation resamples, take mean across repetitions
df_cv <- mod_rf_caret_leafn$pred %>% 
  as_tibble() %>% 
  dplyr::filter(mtry == mod_rf_caret_leafn$bestTune$mtry, 
                splitrule == mod_rf_caret_leafn$bestTune$splitrule, 
                min.node.size == mod_rf_caret_leafn$bestTune$min.node.size) %>%
  separate(Resample, into = c(NA, "Fold"), sep = "old") %>% 
  dplyr::rename(idx = rowIndex)
  # left_join(
  #   dfs %>%
  #     ungroup() %>%
  #     drop_na() %>%
  #     dplyr::select(leafN) %>%
  #     mutate(idx = seq(nrow(.))),
  #   by = "idx"
  #   ) %>%
  # dplyr::select(obs = leafN, mod = pred)

out <- df_cv %>% 
  rbeni::analyse_modobs2("pred", "obs", type = "heat")
out$gg +
  ylim(5,40) + xlim(5,40)
```

